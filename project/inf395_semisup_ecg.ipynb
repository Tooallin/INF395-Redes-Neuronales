{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60bf191f",
   "metadata": {},
   "source": [
    "# **Detectar arritmias card√≠acas mediante se√±ales de ECG parcialmente etiquetadas**\n",
    "### INF395 Introducci√≥n a las Redes Neuronales and Deep Learning\n",
    "- Estudiante: Alessandro Bruno Cintolesi Rodr√≠guez\n",
    "- ROL: 202173541-0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a529d849",
   "metadata": {},
   "source": [
    "## 1. Importaci√≥n de Librer√≠as\n",
    "Importamos todas las librer√≠as necesarias, incluyendo PyTorch, Sklearn, Pandas y Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dab68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === General / Utilidad ===\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === PyTorch / PyTorch Lightning ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# === Scikit-learn ===\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "\tf1_score,\n",
    "\taccuracy_score,\n",
    "\tconfusion_matrix,\n",
    ")\n",
    "\n",
    "from itertools import cycle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35599b73",
   "metadata": {},
   "source": [
    "## 2. Configuraci√≥n Global y Hiperpar√°metros\n",
    "Definimos las variables globales, hiperpar√°metros y seteamos el dispositivo (GPU o CPU) que se usar√° para el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147cc694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seteamos la semilla\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "pl.seed_everything(SEED, workers=True)\n",
    "\n",
    "# Seteamos el dispositivo\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "print(\"Using device:\", DEVICE)\n",
    "if DEVICE.type == \"cuda\":\n",
    "\tprint(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b331f51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.2              # Reservamos el 20% de los datos etiquetados para validar y evitar sobreajuste.\n",
    "BATCH_SIZE = 32              # El modelo procesa 32 ECGs simult√°neamente antes de actualizar sus pesos.\n",
    "EPOCHS = 50                  # Cantidad de veces que el modelo ver√° el set de datos completo durante el entrenamiento.\n",
    "LEARNING_RATE = 1e-3         # Velocidad de aprendizaje; controla qu√© tan r√°pido se ajustan los pesos del modelo.\n",
    "CONFIDENCE_THRESHOLD = 0.95  # (FixMatch) Solo confiamos en las predicciones de datos sin etiqueta si la certeza > 95%.\n",
    "LAMBDA_U = 1.0               # Factor de equilibrio: da igual importancia a la p√©rdida supervisada y a la no supervisada.\n",
    "NUM_CLASSES = 5              # Total de categor√≠as de arritmias (0-4) que el modelo debe aprender a clasificar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711f7774",
   "metadata": {},
   "source": [
    "## 3. Carga y Preprocesamiento de Datos\n",
    "Realizamos los siguientes pasos:\n",
    "1.  **Cargar** los archivos `train_semi_supervised.csv` y `test_semi_supervised.csv`.\n",
    "2.  **Separar** el set de entrenamiento en datos **etiquetados** y **no etiquetados** (basado en `NaN`).\n",
    "3.  **Crear un set de Validaci√≥n** (con `TEST_SIZE`) a partir de los datos *etiquetados*, usando una divisi√≥n estratificada para mantener el balance.\n",
    "4.  **Normalizar** los datos: ajustamos un `StandardScaler` (Z-Score) *solo* con `X_train_labeled` y luego transformamos todos los sets (`X_unlabeled`, `X_val`, `X_test`).\n",
    "5.  **Reformatear** los datos a la forma `(N, 1, 187)` requerida por la 1D-CNN de PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b524e6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Carga de Datos ---\n",
    "try:\n",
    "\t# Cargamos el set de entrenamiento: aqu√≠ est√°n mezclados los datos con etiqueta y los sin etiqueta (NaN).\n",
    "\ttrain_df = pd.read_csv(\"ecg_signals/train_semi_supervised.csv\")\n",
    "\n",
    "\t# Cargamos el set de prueba: datos \"futuros\" que el modelo nunca ver√° durante el entrenamiento.\n",
    "\ttest_df = pd.read_csv(\"ecg_signals/test_semi_supervised.csv\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "\t# Bloque de seguridad: detiene la ejecuci√≥n limpiamente si la ruta de los archivos es incorrecta.\n",
    "\tprint(\"Error: No se encontraron los archivos CSV. Verifica la ruta.\")\n",
    "\texit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9e2fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Forma (shape) original de entrenamiento: {train_df.shape}\")\n",
    "print(f\"Forma (shape) original de testing: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a84a40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Procesamiento del Set de Test ---\n",
    "\n",
    "# Extraemos la se√±al ECG (features): ignoramos la col 0 (ID) y tomamos de la 1 a la 187.\n",
    "X_test_raw = test_df.iloc[:, 1:188].values\n",
    "\n",
    "# Extraemos la etiqueta (target): columna 188, y forzamos que sean enteros para el modelo.\n",
    "y_test_raw = test_df.iloc[:, 188].values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c4b388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Procesamiento del Set de Entrenamiento (SSL) ---\n",
    "\n",
    "# Creamos una \"m√°scara\" l√≥gica: True si la fila tiene etiqueta, False si es NaN (sin etiqueta).\n",
    "labeled_mask = train_df.iloc[:, 187].notna()\n",
    "\n",
    "# Separamos los datos en dos mundos distintos:\n",
    "# 1. labeled_df: Datos que el m√©dico etiquet√≥. Usaremos esto para aprendizaje Supervisado normal.\n",
    "labeled_df = train_df[labeled_mask]\n",
    "\n",
    "# 2. unlabeled_df: Datos sin etiqueta (la mayor√≠a). Usaremos esto con FixMatch para aprender la estructura de la se√±al.\n",
    "unlabeled_df = train_df[~labeled_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a310d5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nDatos etiquetados encontrados: {len(labeled_df)}\")\n",
    "print(f\"Datos NO etiquetados encontrados: {len(unlabeled_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38fc455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Crear X/y para Labeled y Unlabeled ---\n",
    "\n",
    "# Extraemos la matriz de caracter√≠sticas (se√±ales ECG) de los datos etiquetados.\n",
    "X_labeled_full = labeled_df.iloc[:, 0:187].values\n",
    "\n",
    "# Extraemos el vector de etiquetas (diagn√≥sticos) correspondientes. \n",
    "# Es vital usar .astype(int) para que la funci√≥n de p√©rdida de PyTorch lo acepte.\n",
    "y_labeled_full = labeled_df.iloc[:, 187].values.astype(int)\n",
    "\n",
    "# Extraemos solo las caracter√≠sticas de los datos NO etiquetados.\n",
    "# Nota: Aqu√≠ no extraemos 'y' porque no existe (es lo que el modelo intentar√° adivinar).\n",
    "X_unlabeled_raw = unlabeled_df.iloc[:, 0:187].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f08f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Crear Set de Validaci√≥n (Estratificado) ---\n",
    "\n",
    "# Subdividimos los datos etiquetados: \n",
    "# 'X_train_labeled_raw': Para entrenar al modelo.\n",
    "# 'X_val_raw': Para medir su rendimiento en datos no vistos (Validaci√≥n).\n",
    "X_train_labeled_raw, X_val_raw, y_train_labeled, y_val = train_test_split(\n",
    "\tX_labeled_full,\n",
    "\ty_labeled_full,\n",
    "\ttest_size=TEST_SIZE,      # Reservamos el 20% (definido arriba) para validaci√≥n.\n",
    "\tstratify=y_labeled_full,  # Mantiene la misma proporci√≥n de clases (desbalance) en train y val.\n",
    "\trandom_state=SEED         # Semilla fija para que la divisi√≥n sea reproducible siempre igual.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24660382",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Divisi√≥n Final (Etiquetados) ---\")\n",
    "print(f\"Muestras de entrenamiento (etiquetadas): {len(X_train_labeled_raw)}\")\n",
    "print(f\"Muestras de validaci√≥n (etiquetadas): {len(X_val_raw)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a613a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Normalizaci√≥n (StandardScaler) ---\n",
    "\n",
    "# Inicializamos el escalador. Usaremos Z-Score (restar media, dividir por desviaci√≥n est√°ndar).\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Calculamos la media y desviaci√≥n est√°ndar SOLO con los datos de entrenamiento.\n",
    "scaler.fit(X_train_labeled_raw)\n",
    "\n",
    "# Ahora usamos esa calculadora calibrada para transformar TODOS los conjuntos.\n",
    "# Esto asegura que todos los datos est√©n en la misma escala matem√°tica.\n",
    "X_train_labeled_scaled = scaler.transform(X_train_labeled_raw)\n",
    "X_unlabeled_scaled = scaler.transform(X_unlabeled_raw)\n",
    "X_val_scaled = scaler.transform(X_val_raw)\n",
    "X_test_scaled = scaler.transform(X_test_raw)\n",
    "\n",
    "print(\"\\nDatos normalizados (escalados) exitosamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc565124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. Reformatear (Reshape) para 1D-CNN ---\n",
    "\n",
    "# Las capas Conv1d de PyTorch exigen una entrada de 3 dimensiones: (Batch, Canales, Tiempo).\n",
    "# Nuestros datos actuales son 2D: (Muestras, 187).\n",
    "# Usamos np.newaxis para insertar una dimensi√≥n de \"Canal\" (que es 1, ya que es un solo sensor ECG).\n",
    "# La forma cambia de (N, 187) a (N, 1, 187).\n",
    "\n",
    "X_train_labeled = X_train_labeled_scaled[:, np.newaxis, :]\n",
    "X_unlabeled = X_unlabeled_scaled[:, np.newaxis, :]\n",
    "X_val = X_val_scaled[:, np.newaxis, :]\n",
    "X_test = X_test_scaled[:, np.newaxis, :]\n",
    "\n",
    "# Las etiquetas (y) se mantienen igual, ya que son solo una lista de respuestas correctas.\n",
    "y_test = y_test_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3854830",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Forma Final de los Datos (Listos para el Modelo) ---\")\n",
    "print(f\"X_train_labeled: {X_train_labeled.shape}\")\n",
    "print(f\"y_train_labeled: {y_train_labeled.shape}\")\n",
    "print(f\"X_unlabeled: {X_unlabeled.shape}\")\n",
    "print(f\"X_val: {X_val.shape}\")\n",
    "print(f\"y_val: {y_val.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d124cb9e",
   "metadata": {},
   "source": [
    "## 4. An√°lisis Exploratorio (Desbalance)\n",
    "Verificamos el **fuerte desbalance de clases** en nuestros datos etiquetados. Esto confirma la necesidad de usar m√©tricas como F1-Macro y una funci√≥n de p√©rdida especializada como Focal Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c15d38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- An√°lisis de Desbalance de Clases (Datos Etiquetados) ---\")\n",
    "\n",
    "# Contar las clases en el set etiquetado completo\n",
    "class_counts = pd.Series(y_labeled_full).value_counts().sort_index()\n",
    "print(class_counts)\n",
    "\n",
    "# Calcular porcentajes\n",
    "class_percentages = (class_counts / len(y_labeled_full)) * 100\n",
    "print(\"\\nPorcentaje de cada clase:\")\n",
    "print(class_percentages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a7c437",
   "metadata": {},
   "source": [
    "## 5. Definici√≥n del Modelo y Funci√≥n de P√©rdida\n",
    "\n",
    "### 5.1. Modelo 1D-CNN\n",
    "Definimos la arquitectura de nuestra Red Convolucional 1D (`ECG_1D_CNN`), ideal para aprender patrones en series temporales como los ECG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7145949c",
   "metadata": {},
   "source": [
    "![Diagrama Arquitectura ECG](ecg_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2707a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la arquitectura del modelo\n",
    "class ECG_1D_CNN(nn.Module):\n",
    "\tdef __init__(self, num_classes=5):\n",
    "\t\tsuper(ECG_1D_CNN, self).__init__()\n",
    "\t\t\n",
    "\t\t# --- Extractor de Caracter√≠sticas ---\n",
    "\t\t\n",
    "\t\t# Bloque 1: Detecta patrones simples de bajo nivel.\n",
    "\t\tself.conv_block1 = nn.Sequential(\n",
    "\t\t\t# Conv1d: Escanea la se√±al. Entra 1 canal (el ECG), salen 64 mapas de caracter√≠sticas.\n",
    "\t\t\tnn.Conv1d(in_channels=1, out_channels=64, \n",
    "\t\t\t\t\t  kernel_size=5, stride=1, padding=2),\n",
    "\t\t\t# BatchNorm: Normaliza los valores para que el entrenamiento sea estable y r√°pido.\n",
    "\t\t\tnn.BatchNorm1d(64),\n",
    "\t\t\t# ReLU: \"Enciende\" las neuronas si encuentra algo positivo, apaga si es negativo.\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\t# MaxPool: Reduce la se√±al a la mitad (187 -> 93).\n",
    "\t\t\tnn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\t# Bloque 2: Detecta patrones intermedios combinando los anteriores.\n",
    "\t\tself.conv_block2 = nn.Sequential(\n",
    "\t\t\t# Aumentamos la profundidad a 128 canales para capturar m√°s detalles.\n",
    "\t\t\tnn.Conv1d(in_channels=64, out_channels=128, \n",
    "\t\t\t\t\t  kernel_size=5, stride=1, padding=2),\n",
    "\t\t\t# BatchNorm: Normaliza los valores para que el entrenamiento sea estable y r√°pido.\n",
    "\t\t\tnn.BatchNorm1d(128),\n",
    "\t\t\t# ReLU: \"Enciende\" las neuronas si encuentra algo positivo, apaga si es negativo.\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\t# Reducimos la dimensi√≥n temporal otra vez (93 -> 46).\n",
    "\t\t\tnn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\t# Bloque 3: Detecta patrones complejos y abstractos de la arritmia.\n",
    "\t\tself.conv_block3 = nn.Sequential(\n",
    "\t\t\t# M√°xima profundidad (256 canales).\n",
    "\t\t\tnn.Conv1d(in_channels=128, out_channels=256, \n",
    "\t\t\t\t\t  kernel_size=5, stride=1, padding=2),\n",
    "\t\t\t# BatchNorm: Normaliza los valores para que el entrenamiento sea estable y r√°pido.\n",
    "\t\t\tnn.BatchNorm1d(256),\n",
    "\t\t\t# ReLU: \"Enciende\" las neuronas si encuentra algo positivo, apaga si es negativo.\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\t# √öltima reducci√≥n (46 -> 23).\n",
    "\t\t\tnn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\t# --- Cabeza de Clasificaci√≥n ---\n",
    "\t\t\n",
    "\t\t# Flatten: \"Aplana\" el cubo de caracter√≠sticas 3D a un vector largo 1D para poder clasificar.\n",
    "\t\tself.flatten = nn.Flatten() # Transforma (N, 256, 23) -> (N, 5888)\n",
    "\t\t\n",
    "\t\t# Classifier: Red neuronal densa (MLP) que toma la decisi√≥n final.\n",
    "\t\tself.classifier = nn.Sequential(\n",
    "\t\t\tnn.Linear(in_features=256 * 23, out_features=512),\n",
    "\t\t\t# ReLU: \"Enciende\" las neuronas si encuentra algo positivo, apaga si es negativo.\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\t# Dropout: Apaga aleatoriamente el 50% de neuronas para evitar memorizaci√≥n (overfitting).\n",
    "\t\t\tnn.Dropout(0.5),\n",
    "\t\t\t# Capa final: Reduce todo a 5 n√∫meros (las puntuaciones para cada clase).\n",
    "\t\t\tnn.Linear(in_features=512, out_features=num_classes)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t\"\"\"\n",
    "\t\tDefine el flujo de datos: desde la se√±al cruda hasta la predicci√≥n.\n",
    "\t\t\"\"\"\n",
    "\t\t# Pasamos la se√±al por los bloques extractores (convolucionales)\n",
    "\t\tx = self.conv_block1(x)\n",
    "\t\tx = self.conv_block2(x)\n",
    "\t\tx = self.conv_block3(x)\n",
    "\t\t\n",
    "\t\t# Preparamos los datos para la clasificaci√≥n\n",
    "\t\tx = self.flatten(x)\n",
    "\t\t\n",
    "\t\t# Obtenemos los 'logits' (puntuaciones crudas) de las 5 clases\n",
    "\t\tlogits = self.classifier(x)\n",
    "\t\treturn logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d346bf68",
   "metadata": {},
   "source": [
    "### 5.2. Focal Loss\n",
    "Definimos la clase `FocalLoss`. Esta p√©rdida modificada nos ayudar√° a mitigar el desbalance de clases forzando al modelo a enfocarse en las muestras dif√≠ciles (clases minoritarias)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556d1b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "\tdef __init__(self, gamma=2.0, alpha=None, reduction='mean'):\n",
    "\t\tsuper(FocalLoss, self).__init__()\n",
    "\t\tself.gamma = gamma  # El \"foco\": cu√°nto ignoramos los ejemplos f√°ciles (gamma=0 es CrossEntropy normal).\n",
    "\t\tself.alpha = alpha  # El \"balance\": pesos manuales para dar m√°s importancia a clases minoritarias.\n",
    "\t\tself.reduction = reduction\n",
    "\t\t\n",
    "\t\tif self.alpha is not None:\n",
    "\t\t\tif not isinstance(self.alpha, torch.Tensor):\n",
    "\t\t\t\tself.alpha = torch.tensor(self.alpha)\n",
    "\t\t\t\t\n",
    "\tdef forward(self, logits, targets):\n",
    "\t\t\"\"\"\n",
    "\t\tCalcula la p√©rdida Focal.\n",
    "\t\tInput: logits (Predicciones crudas del modelo), targets (Respuestas correctas)\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\t# 1. Log-Softmax: Convertimos los puntajes crudos en probabilidades logar√≠tmicas.\n",
    "\t\tlog_probs = F.log_softmax(logits, dim=1)\n",
    "\t\t\n",
    "\t\t# 2. Extraer la probabilidad de la clase correcta (p_t):\n",
    "\t\t# De las 5 probabilidades que da el modelo, 'gather' selecciona SOLO la que corresponde a la etiqueta real.\n",
    "\t\tlog_pt_true = log_probs.gather(1, targets.view(-1, 1)).view(-1)\n",
    "\t\t\n",
    "\t\t# 3. Obtener la probabilidad real (pt):\n",
    "\t\t# Deshacemos el logaritmo para tener un valor entre 0 y 1.\n",
    "\t\tpt_true = log_pt_true.exp()\n",
    "\t\t\n",
    "\t\t# 4. Calcular el Factor de Modulaci√≥n: (1 - pt)^gamma\n",
    "\t\t# - Si el modelo est√° muy seguro (pt -> 1), (1-pt) es casi 0 -> La p√©rdida se anula (lo ignoramos).\n",
    "\t\t# - Si el modelo se equivoca (pt -> 0), (1-pt) es casi 1 -> La p√©rdida se mantiene alta (aprendemos de esto).\n",
    "\t\tfocal_term = (1 - pt_true)**self.gamma\n",
    "\t\t\n",
    "\t\t# 5. P√©rdida Base (Cross Entropy):\n",
    "\t\t# Calculamos la entrop√≠a cruzada est√°ndar (-log(pt)).\n",
    "\t\tce_loss = -log_pt_true\n",
    "\t\t\n",
    "\t\t# 6. Combinaci√≥n: Multiplicamos la p√©rdida est√°ndar por nuestro factor de \"foco\".\n",
    "\t\tloss = focal_term * ce_loss\n",
    "\t\t\n",
    "\t\t# 7. Aplicar pesos Alpha (Opcional):\n",
    "\t\t# Si queremos forzar a√∫n m√°s el balance, multiplicamos por el peso espec√≠fico de cada clase.\n",
    "\t\tif self.alpha is not None:\n",
    "\t\t\tif self.alpha.device != logits.device:\n",
    "\t\t\t\tself.alpha = self.alpha.to(logits.device)\n",
    "\t\t\t\n",
    "\t\t\talpha_t = self.alpha.gather(0, targets)\n",
    "\t\t\tloss = alpha_t * loss\n",
    "\t\t\t\n",
    "\t\t# 8. Reducci√≥n: Devolvemos el promedio del error de todo el lote (batch).\n",
    "\t\tif self.reduction == 'mean':\n",
    "\t\t\treturn loss.mean()\n",
    "\t\telif self.reduction == 'sum':\n",
    "\t\t\treturn loss.sum()\n",
    "\t\telse:\n",
    "\t\t\treturn loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec4b651",
   "metadata": {},
   "source": [
    "## 6. Definici√≥n de Aumentaciones (FixMatch)\n",
    "Definimos las dos funciones de aumentaci√≥n requeridas por FixMatch:\n",
    "* `aug_weak`: A√±ade ruido gaussiano leve.\n",
    "* `aug_strong`: A√±ade ruido fuerte y \"time masking\" (secciones puestas a cero).\n",
    "\n",
    "Luego, visualizamos un ejemplo para confirmar su efecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8a1a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. AUMENTACI√ìN D√âBIL ---\n",
    "\n",
    "# Definimos la funci√≥n de aumentaci√≥n d√©bil.\n",
    "# Objetivo: Simular peque√±as variaciones naturales en la se√±al (como ruido del sensor o movimiento leve)\n",
    "# para que el modelo sea robusto y no se \"memorice\" los valores exactos de cada p√≠xel.\n",
    "def aug_weak(x_batch, noise_level=0.01):\n",
    "\t\"\"\"\n",
    "\tAplica una aumentaci√≥n d√©bil a√±adiendo ruido gaussiano.\n",
    "\tInput: x_batch (N, 1, 187)\n",
    "\t\"\"\"\n",
    "\t# Paso de seguridad: Creamos una copia independiente de los datos originales.\n",
    "\tx_aug = x_batch.clone()\n",
    "\t\n",
    "\t# Generamos el \"ruido\": una matriz de n√∫meros aleatorios (distribuci√≥n normal) con la misma forma que nuestros ECGs.\n",
    "\t# Multiplicamos por 'noise_level' (0.01) para mantener el ruido muy suave y no destruir la informaci√≥n cl√≠nica.\n",
    "\tnoise = torch.randn_like(x_aug) * noise_level\n",
    "\t\n",
    "\t# Sumamos el ruido a la se√±al original.\n",
    "\treturn x_aug + noise.to(x_batch.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203e42b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_strong(x_batch, noise_level=0.1, num_masks=3, mask_size=20):\n",
    "\t\"\"\"\n",
    "\tAplica aumentaci√≥n fuerte: combina ruido intenso y borrado de secciones (Time Masking).\n",
    "\tInput: x_batch (N, 1, 187)\n",
    "\t\"\"\"\n",
    "\t# Paso de seguridad: Creamos una copia independiente de los datos originales.\n",
    "\tx_aug = x_batch.clone()\n",
    "\t\n",
    "\t# 1. Ruido Fuerte:\n",
    "\t# A√±adimos 10 veces m√°s ruido que en la versi√≥n d√©bil (0.1 vs 0.01).\n",
    "\tnoise = torch.randn_like(x_aug) * noise_level\n",
    "\tx_aug += noise.to(x_batch.device)\n",
    "\t\n",
    "\t# 2. Time Masking:\n",
    "\t# Vamos a \"borrar\" o poner a cero partes aleatorias del electrocardiograma.\n",
    "\t# Esto obliga al modelo a usar el contexto: debe adivinar la arritmia viendo solo fragmentos de la se√±al.\n",
    "\tN, C, L = x_aug.shape # N=Batch, C=1, L=187\n",
    "\t\n",
    "\tfor i in range(N): # Procesamos cada paciente del lote uno por uno\n",
    "\t\tfor _ in range(num_masks): # Aplicamos 3 cortes distintos por paciente\n",
    "\t\t\t# Elegimos un punto de inicio al azar, asegur√°ndonos de no salirnos del final de la se√±al.\n",
    "\t\t\tt_start = torch.randint(0, L - mask_size, (1,)).item()\n",
    "\t\t\t\n",
    "\t\t\t# Ponemos a cero una ventana de 20 puntos.\n",
    "\t\t\tx_aug[i, :, t_start : t_start + mask_size] = 0.0\n",
    "\t\t\t\n",
    "\treturn x_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9274b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ecg_comparison(signals_list, titles_list):\n",
    "\t\"\"\"\n",
    "\tGrafica una lista de se√±ales de ECG en subplots verticales para comparar.\n",
    "\tMUESTRA el gr√°fico en lugar de guardarlo.\n",
    "\n",
    "\tArgs:\n",
    "\t\tsignals_list (list): Lista de tensores de ECG.\n",
    "\t\ttitles_list (list): Lista de strings para los t√≠tulos de cada subplot.\n",
    "\t\"\"\"\n",
    "\tif len(signals_list) != len(titles_list):\n",
    "\t\tprint(\"Error: El n√∫mero de se√±ales no coincide con el n√∫mero de t√≠tulos.\")\n",
    "\t\treturn\n",
    "\n",
    "\tnum_signals = len(signals_list)\n",
    "\n",
    "\tfig, axes = plt.subplots(nrows=num_signals, ncols=1, \n",
    "\t\t\t\t\t\t\t\tfigsize=(15, 3 * num_signals), \n",
    "\t\t\t\t\t\t\t\tsharex=True)\n",
    "\n",
    "\tif num_signals == 1:\n",
    "\t\taxes = [axes]\n",
    "\n",
    "\tfor i in range(num_signals):\n",
    "\t\tax = axes[i]\n",
    "\t\t\n",
    "\t\tsignal_np = signals_list[i].cpu().detach().numpy().squeeze()\n",
    "\t\t\n",
    "\t\tax.plot(signal_np)\n",
    "\t\tax.set_title(titles_list[i], fontsize=14)\n",
    "\t\tax.set_ylabel(\"Amplitud\")\n",
    "\t\tax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "\taxes[-1].set_xlabel(\"Paso de Tiempo (Time Step)\")\n",
    "\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e308a573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ejemplo de uso ---\n",
    "dummy_input = torch.randn(4, 1, 187).to(DEVICE)\n",
    "\n",
    "weak_output = aug_weak(dummy_input)\n",
    "strong_output = aug_strong(dummy_input)\n",
    "\n",
    "sample_original = dummy_input[0]\n",
    "sample_weak = weak_output[0]\n",
    "sample_strong = strong_output[0]\n",
    "\n",
    "plot_ecg_comparison(\n",
    "\tsignals_list=[sample_original, sample_weak, sample_strong],\n",
    "\ttitles_list=[\"1. ECG Original\", \"2. Aumentaci√≥n D√©bil (Ruido)\", \"3. Aumentaci√≥n Fuerte (Ruido + Masking)\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c72f58",
   "metadata": {},
   "source": [
    "## 7. Creaci√≥n de Datasets y DataLoaders\n",
    "Definimos las clases `LabeledECGDataset` y `UnlabeledECGDataset` para manejar nuestros datos. Luego, creamos los `DataLoader` para el entrenamiento (etiquetado y no etiquetado) y la validaci√≥n/testeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe1938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Definir los Datasets ---\n",
    "class LabeledECGDataset(Dataset):\n",
    "\t\"\"\"Dataset para nuestros datos etiquetados (Entrenamiento Supervisado y Validaci√≥n)\"\"\"\n",
    "\tdef __init__(self, x_data, y_data):\n",
    "\t\t# Recibimos los datos (X) y las etiquetas (y).\n",
    "\t\tself.x_data = x_data \n",
    "\t\tself.y_data = y_data\n",
    "\t\t\n",
    "\tdef __len__(self):\n",
    "\t\t# Le dice a PyTorch cu√°ntas muestras totales existen.\n",
    "\t\treturn len(self.x_data)\n",
    "\t\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\t# Convertimos a Tensor cada vez que el modelo pide una muestra espec√≠fica.\n",
    "\t\tx = torch.tensor(self.x_data[idx], dtype=torch.float32) # La se√±al\n",
    "\t\ty = torch.tensor(self.y_data[idx], dtype=torch.long)    # La etiqueta (0-4)\n",
    "\t\treturn x, y\n",
    "\n",
    "class UnlabeledECGDataset(Dataset):\n",
    "\t\"\"\"Dataset para datos NO etiquetados (Solo para FixMatch)\"\"\"\n",
    "\tdef __init__(self, x_data):\n",
    "\t\t# Aqu√≠ solo recibimos X, porque no existen etiquetas (y).\n",
    "\t\tself.x_data = x_data\n",
    "\t\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.x_data)\n",
    "\t\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\t# Convertimos a Tensor la se√±al individual.\n",
    "\t\tx = torch.tensor(self.x_data[idx], dtype=torch.float32)\n",
    "\t\t# Solo devolvemos X. El modelo tendr√° que \"imaginar\" la etiqueta (pseudo-label).\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab684ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Crear Datasets y DataLoaders ---\n",
    "\n",
    "# --- Datasets ---\n",
    "# Envolvemos nuestras matrices num√©ricas en objetos 'Dataset'.\n",
    "train_labeled_ds = LabeledECGDataset(X_train_labeled, y_train_labeled)\n",
    "train_unlabeled_ds = UnlabeledECGDataset(X_unlabeled)\n",
    "val_ds = LabeledECGDataset(X_val, y_val)\n",
    "test_ds = LabeledECGDataset(X_test, y_test)\n",
    "\n",
    "# --- DataLoaders ---\n",
    "# Para ENTRENAMIENTO (labeled y unlabeled): shuffle=True\n",
    "labeled_loader = DataLoader(train_labeled_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "unlabeled_loader = DataLoader(train_unlabeled_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Para VALIDACI√ìN y TEST: shuffle=False\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6397e3",
   "metadata": {},
   "source": [
    "## 8. Funciones de Entrenamiento y Evaluaci√≥n\n",
    "\n",
    "### 8.1. train_fixmatch\n",
    "Esta es la funci√≥n principal que implementa la l√≥gica de FixMatch en cada √©poca:\n",
    "1.  Calcula la p√©rdida supervisada (`loss_s`) en el batch etiquetado.\n",
    "2.  Genera pseudo-etiquetas en el batch no etiquetado (con `aug_weak`).\n",
    "3.  Filtra las pseudo-etiquetas usando el `CONFIDENCE_THRESHOLD`.\n",
    "4.  Calcula la p√©rdida de consistencia (`loss_u`) usando `aug_strong`.\n",
    "5.  Combina las p√©rdidas (`loss_s + LAMBDA_U * loss_u`) y retropropaga.\n",
    "\n",
    "### 8.2. evaluate\n",
    "Funci√≥n est√°ndar para evaluar el modelo en el set de validaci√≥n o test. Devuelve el **F1-Macro Score** (nuestra m√©trica clave) y la matriz de confusi√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916b77cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fixmatch(model, labeled_loader, unlabeled_loader, optimizer, loss_sup, loss_unsup, device, conf_threshold, lambda_u):\n",
    "\t\"\"\"\n",
    "\tEjecuta una √©poca de entrenamiento. Aqu√≠ es donde ocurre la l√≥gica de FixMatch.\n",
    "\t\"\"\"\n",
    "\tmodel.train() # Activamos modo entrenamiento\n",
    "\t\n",
    "\ttotal_loss_s = 0.0\n",
    "\ttotal_loss_u = 0.0\n",
    "\t\n",
    "\t# Como tenemos muchos m√°s datos NO etiquetados que etiquetados, 'cycle' reinicia el iterador \n",
    "\t# de los no etiquetados infinitamente para que nunca nos quedemos sin ellos \n",
    "\t# mientras recorremos una vez el set etiquetado.\n",
    "\tpbar = tqdm(zip(labeled_loader, cycle(unlabeled_loader)), total=len(labeled_loader))\n",
    "\t\n",
    "\tfor (x_labeled, y_labeled), x_unlabeled in pbar:\n",
    "\t\t\n",
    "\t\t# Movemos todo a la GPU para velocidad\n",
    "\t\tx_labeled = x_labeled.to(device)\n",
    "\t\ty_labeled = y_labeled.to(device)\n",
    "\t\tx_unlabeled = x_unlabeled.to(device)\n",
    "\t\t\n",
    "\t\t# --- 1. Parte Supervisada (Lo cl√°sico) ---\n",
    "\t\t# Entrenamos con los datos que S√ç tienen etiqueta.\n",
    "\t\t# Usamos aumentaci√≥n d√©bil para robustez b√°sica.\n",
    "\t\tx_labeled_aug = aug_weak(x_labeled)\n",
    "\t\tlogits_s = model(x_labeled_aug)\n",
    "\t\tloss_s = loss_sup(logits_s, y_labeled) \n",
    "\t\t\n",
    "\t\t# --- 2. Parte No Supervisada (La magia de FixMatch) ---\n",
    "\t\t# Crear la Pseudo-Etiqueta (El \"Maestro\")\n",
    "\t\t# Usamos aumentaci√≥n D√âBIL y NO calculamos gradientes.\n",
    "\t\twith torch.no_grad(): \n",
    "\t\t\tx_unlabeled_w = aug_weak(x_unlabeled)\n",
    "\t\t\tlogits_uw = model(x_unlabeled_w)\n",
    "\t\t\t\n",
    "\t\t\t# Calculamos la probabilidad de la predicci√≥n\n",
    "\t\t\tprobs_uw = torch.softmax(logits_uw, dim=1)\n",
    "\t\t\tmax_prob, pseudo_label = torch.max(probs_uw, dim=1)\n",
    "\t\t\t\n",
    "\t\t\t# Filtro de Confianza: Si el modelo duda (probabilidad < 95%),\n",
    "\t\t\t# creamos una m√°scara de 0 para ignorar este dato m√°s adelante.\n",
    "\t\t\tmask = (max_prob >= conf_threshold).float()\n",
    "\t\t\t\n",
    "\t\t# Ahora tomamos la MISMA se√±al, le aplicamos aumentaci√≥n FUERTE (dif√≠cil),\n",
    "\t\t# y forzamos al modelo a predecir lo mismo que predijo en la versi√≥n f√°cil.\n",
    "\t\tx_unlabeled_s = aug_strong(x_unlabeled)\n",
    "\t\tlogits_us = model(x_unlabeled_s)\n",
    "\t\t\n",
    "\t\t# Calculamos el error entre la predicci√≥n dif√≠cil y la pseudo-etiqueta f√°cil.\n",
    "\t\tloss_u_all = loss_unsup(logits_us, pseudo_label)\n",
    "\t\t\n",
    "\t\t# Aplicamos la m√°scara: Solo aprendemos de los casos donde el modelo estaba seguro.\n",
    "\t\tloss_u = (loss_u_all * mask).mean()\n",
    "\t\t\n",
    "\t\t# --- 3. Optimizaci√≥n ---\n",
    "\t\t# Sumamos ambas p√©rdidas. Lambda_u controla el peso de la parte no supervisada.\n",
    "\t\ttotal_loss = loss_s + lambda_u * loss_u\n",
    "\t\t\n",
    "\t\toptimizer.zero_grad()   # Limpiar basura anterior\n",
    "\t\ttotal_loss.backward()   # Calcular gradientes\n",
    "\t\toptimizer.step()        # Actualizar pesos\n",
    "\t\t\n",
    "\t\ttotal_loss_s += loss_s.item()\n",
    "\t\ttotal_loss_u += loss_u.item()\n",
    "\t\t\n",
    "\t\tpbar.set_description(f\"Loss_S: {loss_s.item():.4f} | Loss_U: {loss_u.item():.4f}\")\n",
    "\t\t\n",
    "\treturn total_loss_s / len(labeled_loader), total_loss_u / len(labeled_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f26f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader, device):\n",
    "\t\"\"\"\n",
    "\tEval√∫a el modelo en el set de validaci√≥n.\n",
    "\t\"\"\"\n",
    "\t# --- 1. Modo Evaluaci√≥n ---\n",
    "\tmodel.eval() \n",
    "\t\n",
    "\tall_preds = []\n",
    "\tall_targets = []\n",
    "\t\n",
    "\t# --- 2. Optimizaci√≥n (Sin Gradientes) ---\n",
    "\twith torch.no_grad(): \n",
    "\t\tfor x_val, y_val in val_loader:\n",
    "\t\t\tx_val = x_val.to(device)\n",
    "\t\t\ty_val = y_val.to(device)\n",
    "\t\t\t\n",
    "\t\t\tlogits = model(x_val)\n",
    "\t\t\t\n",
    "\t\t\t# --- 3. Decisi√≥n Final (Argmax) ---\n",
    "\t\t\tpreds = torch.argmax(logits, dim=1)\n",
    "\t\t\t\n",
    "\t\t\t# Movemos los datos de la GPU a la CPU para que Scikit-learn pueda leerlos.\n",
    "\t\t\tall_preds.extend(preds.cpu().numpy())\n",
    "\t\t\tall_targets.extend(y_val.cpu().numpy())\n",
    "\t\t\t\n",
    "\t# --- 4. M√©tricas Clave ---\n",
    "\t# F1-Macro: Promedia el √©xito de cada clase por separado. \n",
    "\tf1_macro = f1_score(all_targets, all_preds, average='macro', zero_division=0)\n",
    "\t\n",
    "\t# Accuracy: El porcentaje total de aciertos.\n",
    "\tacc = accuracy_score(all_targets, all_preds)\n",
    "\t\n",
    "\t# Matriz de Confusi√≥n: Nos permite ver exactamente d√≥nde se equivoca.\n",
    "\tcm = confusion_matrix(all_targets, all_preds)\n",
    "\t\n",
    "\treturn acc, f1_macro, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af3e3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Inicializar Modelo, P√©rdida y Optimizador ---\n",
    "model = ECG_1D_CNN(num_classes=NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "# --- Definici√≥n de Funciones de P√©rdida ---\n",
    "\n",
    "# (Supervisado): Calcula el error promedio del batch.\n",
    "# reduction='mean': Usado para los datos que TIENEN etiqueta real.\n",
    "loss_sup = FocalLoss(gamma=2.0, alpha=None, reduction='mean').to(DEVICE)\n",
    "\n",
    "# (No Supervisado): Calcula el error individual por muestra.\n",
    "# reduction='none': Necesitamos el error individual para poder multiplicar por la m√°scara (0 o 1)\n",
    "# y as√≠ anular el error de las muestras donde el modelo no estaba seguro.\n",
    "loss_unsup = FocalLoss(gamma=2.0, alpha=None, reduction='none').to(DEVICE)\n",
    "\n",
    "# El Optimizador (Adam): Recibe los par√°metros del modelo y la tasa de aprendizaje (qu√© tan r√°pido debe hacer cambios).\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbab6a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Bucle de Entrenamiento ---\n",
    "\n",
    "best_f1 = -1.0 \n",
    "best_model_path = f\"checkpoints/best_model_ecg.pth\"\n",
    "\n",
    "# Bucle Principal\n",
    "for epoch in range(EPOCHS):\n",
    "\tprint(f\"\\n--- √âpoca {epoch+1}/{EPOCHS} ---\")\n",
    "\t\n",
    "\t# 1. Fase de Aprendizaje (Train):\n",
    "\t# El modelo ve los datos, hace predicciones, calcula el error y ajusta sus pesos.\n",
    "\tavg_loss_s, avg_loss_u = train_fixmatch(\n",
    "\t\tmodel, \n",
    "\t\tlabeled_loader, \n",
    "\t\tunlabeled_loader, \n",
    "\t\toptimizer, \n",
    "\t\tloss_sup, \n",
    "\t\tloss_unsup, \n",
    "\t\tDEVICE, \n",
    "\t\tCONFIDENCE_THRESHOLD, \n",
    "\t\tLAMBDA_U\n",
    "\t)\n",
    "\t\n",
    "\tprint(f\"P√©rdida promedio (S): {avg_loss_s:.4f} | P√©rdida promedio (U): {avg_loss_u:.4f}\")\n",
    "\t\n",
    "\t# 2. Fase de Validaci√≥n (Eval):\n",
    "\t# Evaluamos al modelo en datos que NO ha visto durante el entrenamiento.\n",
    "\tval_acc, val_f1, val_cm = evaluate(model, val_loader, DEVICE)\n",
    "\tprint(f\"Validaci√≥n - Acc: {val_acc:.4f} | F1-Macro: {val_f1:.4f}\")\n",
    "\t\n",
    "\t# 3. Checkpointing:\n",
    "\t# Solo guardamos el modelo en el disco si su F1-Macro es MEJOR que el mejor r√©cord hist√≥rico.\n",
    "\tif val_f1 > best_f1:\n",
    "\t\tbest_f1 = val_f1\n",
    "\t\ttorch.save(model.state_dict(), best_model_path)\n",
    "\t\tprint(f\"üéâ Nuevo mejor modelo guardado ({best_model_path})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbf413c",
   "metadata": {},
   "source": [
    "## 9. Evaluaci√≥n Final en Test Set\n",
    "Cargamos el mejor modelo encontrado en el paso anterior y lo evaluamos contra el **conjunto de test** (que el modelo nunca ha visto) para obtener nuestras m√©tricas de rendimiento finales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cb3505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Evaluaci√≥n Final (con el set de Test) ---\n",
    "print(\"\\n--- Entrenamiento Finalizado ---\")\n",
    "print(\"Cargando mejor modelo para evaluaci√≥n final...\")\n",
    "\n",
    "# Cargamos el modelo 'best_model_path' que guardamos cuando obtuvo su mejor nota en validaci√≥n.\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "# Ejecutamos la evaluaci√≥n sobre el Test Loader.\n",
    "test_acc, test_f1, test_cm = evaluate(model, test_loader, DEVICE)\n",
    "\n",
    "print(f\"\\n--- Resultados Finales (Test Set) ---\")\n",
    "\n",
    "# Reportamos Accuracy: % total de aciertos.\n",
    "print(f\"Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Reportamos F1-Macro.\n",
    "print(f\"F1-Macro: {test_f1:.4f}\")\n",
    "\n",
    "print(\"Matriz de Confusi√≥n (Test):\")\n",
    "# Imprimimos la radiograf√≠a de los errores para analizar: ¬øQu√© clases est√° confundiendo entre s√≠?\n",
    "print(test_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0e2a41",
   "metadata": {},
   "source": [
    "## 10. Generaci√≥n de Archivo de Submission\n",
    "Finalmente, usamos el mejor modelo para generar las predicciones del archivo `test.csv` y las guardamos en el formato `submission.csv` requerido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400aed9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generaci√≥n del Archivo de Submission (Test) ---\n",
    "print(\"\\n--- Generando archivo CSV de submission ---\")\n",
    "\n",
    "# Cargar el mejor modelo guardado\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "# --- 1. Cargar los IDs del CSV de test ---\n",
    "try:\n",
    "\ttest_df_original = pd.read_csv(\"ecg_signals/test_semi_supervised.csv\")\n",
    "\ttest_ids = test_df_original.iloc[:, 0].values\n",
    "\tprint(f\"IDs de test cargados: {len(test_ids)} encontrados.\")\n",
    "except FileNotFoundError:\n",
    "\tprint(\"Error: No se pudo cargar 'test_semi_supervised.csv' para obtener los IDs.\")\n",
    "\texit()\n",
    "\t\n",
    "# --- 2. Obtener todas las predicciones del modelo ---\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "\t# Iteramos sobre el test_loader (que tiene shuffle=False)\n",
    "\tfor x_batch, y_true_labels in test_loader:\n",
    "\t\tx_batch = x_batch.to(DEVICE)\n",
    "\t\t\n",
    "\t\t# Obtener logits\n",
    "\t\tlogits = model(x_batch)\n",
    "\t\t\n",
    "\t\t# Obtener la predicci√≥n (la clase con mayor logit)\n",
    "\t\tpreds = torch.argmax(logits, dim=1)\n",
    "\t\t\n",
    "\t\t# Guardar las predicciones\n",
    "\t\tall_predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "print(f\"Predicciones generadas: {len(all_predictions)} hechas.\")\n",
    "\n",
    "# --- 3. Verificar y Crear el DataFrame ---\n",
    "if len(test_ids) != len(all_predictions):\n",
    "\tprint(f\"¬°Error! La cantidad de IDs ({len(test_ids)}) no coincide \"\n",
    "\t\t  f\"con la cantidad de predicciones ({len(all_predictions)}).\")\n",
    "\tprint(\"Verifica que el test_loader NO est√© barajando (shuffle=False).\")\n",
    "else:\n",
    "\t# Crear el DataFrame con el formato solicitado\n",
    "\tsubmission_df = pd.DataFrame({\n",
    "\t\t'ID': test_ids,\n",
    "\t\t'label': all_predictions\n",
    "\t})\n",
    "\t\n",
    "\t# --- 4. Guardar en CSV ---\n",
    "\toutput_filename = f\"ecg_submittions/submission_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.csv\"\n",
    "\t\n",
    "\t# index=False es crucial para que no a√±ada una columna de √≠ndice\n",
    "\tsubmission_df.to_csv(output_filename, index=False)\n",
    "\t\n",
    "\tprint(f\"\\n¬°√âxito! Archivo de submission guardado en: {output_filename}\")\n",
    "\tprint(\"Vista previa de los resultados:\")\n",
    "\tprint(submission_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3036cd4",
   "metadata": {},
   "source": [
    "## 11. B√∫squeda de Hiperpar√°metros (Grid Search)\n",
    "Usamos `ParameterGrid` de Sklearn para definir una grilla de hiperpar√°metros a probar (Learning Rate, Threshold, Lambda, Gamma).\n",
    "\n",
    "Iteramos sobre cada combinaci√≥n, entrenamos un modelo desde cero y guardamos el `F1-Macro` de validaci√≥n. Finalmente, seleccionamos la combinaci√≥n con el mejor F1-Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c9a671",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\t# === 1. Setup B√°sico y Carga de Datos ===\n",
    "\n",
    "\tSEED = 42\n",
    "\ttorch.manual_seed(SEED)\n",
    "\tnp.random.seed(SEED)\n",
    "\tpl.seed_everything(SEED, workers=True)\n",
    "\tDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\tprint(\"Using device:\", DEVICE)\n",
    "\n",
    "\t# --- 1.1 Carga de Datos ---\n",
    "\ttry:\n",
    "\t\ttrain_df = pd.read_csv(\"ecg_signals/train_semi_supervised.csv\")\n",
    "\t\ttest_df = pd.read_csv(\"ecg_signals/test_semi_supervised.csv\")\n",
    "\texcept FileNotFoundError:\n",
    "\t\tprint(\"Error: No se encontraron los archivos CSV. Verifica la ruta.\")\n",
    "\t\texit()\n",
    "\t# --- 1.2 Procesamiento del Set de Test ---\n",
    "\t# Col 0: ID (lo ignoramos)\n",
    "\t# Col 1-187: Se√±al (Total 187 pasos)\n",
    "\t# Col 188: Clase\n",
    "\tX_test_raw = test_df.iloc[:, 1:188].values  # Columnas 1 a 187\n",
    "\ty_test_raw = test_df.iloc[:, 188].values.astype(int)\n",
    "\n",
    "\t# --- 1.3 Procesamiento del Set de Entrenamiento (SSL) ---\n",
    "\t# Col 0-186: Se√±al (Total 187 pasos)\n",
    "\t# Col 187: Clase\n",
    "\tlabeled_mask = train_df.iloc[:, 187].notna()\n",
    "\n",
    "\t# Separar los DataFrames\n",
    "\tlabeled_df = train_df[labeled_mask]\n",
    "\tunlabeled_df = train_df[~labeled_mask]\n",
    "\t\n",
    "\t# --- 1.4 Crear X/y para Labeled y Unlabeled ---\n",
    "\t# Datos Llenos Etiquetados (para crear train/val)\n",
    "\tX_labeled_full = labeled_df.iloc[:, 0:187].values\n",
    "\ty_labeled_full = labeled_df.iloc[:, 187].values.astype(int)\n",
    "\n",
    "\t# Datos No Etiquetados (solo X)\n",
    "\tX_unlabeled_raw = unlabeled_df.iloc[:, 0:187].values\n",
    "\n",
    "\t# --- 1.5 Crear Set de Validaci√≥n (Estratificado) ---\n",
    "\t# Dividimos el set ETIQUETADO para crear un set de validaci√≥n.\n",
    "\tX_train_labeled_raw, X_val_raw, y_train_labeled, y_val = train_test_split(\n",
    "\t\tX_labeled_full,\n",
    "\t\ty_labeled_full,\n",
    "\t\ttest_size=TEST_SIZE,\n",
    "\t\tstratify=y_labeled_full,\n",
    "\t\trandom_state=SEED\n",
    "\t)\n",
    "\n",
    "\t# --- 1.6 Normalizaci√≥n (StandardScaler) ---\n",
    "\t# Crear y \"ajustar\" (fit) el scaler SOLO con datos de entrenamiento\n",
    "\tscaler = StandardScaler()\n",
    "\tscaler.fit(X_train_labeled_raw)\n",
    "\n",
    "\t# Aplicar \"transform\" a TODOS los sets de datos\n",
    "\tX_train_labeled_scaled = scaler.transform(X_train_labeled_raw)\n",
    "\tX_unlabeled_scaled = scaler.transform(X_unlabeled_raw)\n",
    "\tX_val_scaled = scaler.transform(X_val_raw)\n",
    "\tX_test_scaled = scaler.transform(X_test_raw)\n",
    "\t\n",
    "\t# --- 1.7 Reformatear (Reshape) para 1D-CNN ---\n",
    "\tX_train_labeled = X_train_labeled_scaled[:, np.newaxis, :]\n",
    "\tX_unlabeled = X_unlabeled_scaled[:, np.newaxis, :]\n",
    "\tX_val = X_val_scaled[:, np.newaxis, :]\n",
    "\tX_test = X_test_scaled[:, np.newaxis, :]\n",
    "\n",
    "\ty_test = y_test_raw\n",
    "\tprint(\"\\n--- Carga de datos completada ---\")\n",
    "\n",
    "\t# === 2. Definici√≥n de la B√∫squeda de Hiperpar√°metros ===\n",
    "\n",
    "\t# Define los par√°metros que quieres probar\n",
    "\tparam_grid = {\n",
    "\t\t'LEARNING_RATE': [1e-3, 5e-4],\n",
    "\t\t'CONFIDENCE_THRESHOLD': [0.9, 0.95],\n",
    "\t\t'LAMBDA_U': [1.0, 0.75],\n",
    "\t\t'FOCAL_GAMMA': [2.0, 3.0]\n",
    "\t}\n",
    "\n",
    "\tgrid = ParameterGrid(param_grid)\n",
    "\n",
    "\tall_results = []\n",
    "\n",
    "\t# === 3. Bucle Principal de B√∫squeda (Outer Loop) ===\n",
    "\n",
    "\tprint(f\"\\nIniciando b√∫squeda de par√°metros... {len(list(grid))} combinaciones a probar.\")\n",
    "\n",
    "\tfor run_id, params in enumerate(grid):\n",
    "\t\tprint(f\"\\n--- [RUN {run_id+1}/{len(list(grid))}] ---\")\n",
    "\t\tprint(f\"Par√°metros: {params}\")\n",
    "\n",
    "\t\t# Extraer par√°metros de esta \"run\"\n",
    "\t\tCURRENT_LR = params['LEARNING_RATE']\n",
    "\t\tCURRENT_THRESHOLD = params['CONFIDENCE_THRESHOLD']\n",
    "\t\tCURRENT_LAMBDA_U = params['LAMBDA_U']\n",
    "\t\tCURRENT_GAMMA = params['FOCAL_GAMMA']\n",
    "\t\t\n",
    "\t\t# Hiperpar√°metros fijos\n",
    "\t\tBATCH_SIZE = 32\n",
    "\t\tEPOCHS = 50 # Puedes bajar esto para pruebas r√°pidas\n",
    "\t\tNUM_CLASSES = 5\n",
    "\t\t\n",
    "\t\t### CORRECCI√ìN: Como pediste, NUM_WORKERS = 0 ###\n",
    "\t\tNUM_WORKERS = 0 \n",
    "\t\tPIN_MEMORY = (DEVICE.type == 'cuda')\n",
    "\n",
    "\t\t# === 4. DataLoaders (Sin cambios) ===\n",
    "\t\ttrain_labeled_ds = LabeledECGDataset(X_train_labeled, y_train_labeled)\n",
    "\t\ttrain_unlabeled_ds = UnlabeledECGDataset(X_unlabeled)\n",
    "\t\tval_ds = LabeledECGDataset(X_val, y_val)\n",
    "\t\ttest_ds = LabeledECGDataset(X_test, y_test)\n",
    "\n",
    "\t\tlabeled_loader = DataLoader(\n",
    "\t\t\ttrain_labeled_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "\t\t\tnum_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "\t\t)\n",
    "\t\tunlabeled_loader = DataLoader(\n",
    "\t\t\ttrain_unlabeled_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "\t\t\tnum_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "\t\t)\n",
    "\t\tval_loader = DataLoader(\n",
    "\t\t\tval_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "\t\t\tnum_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "\t\t)\n",
    "\t\ttest_loader = DataLoader(\n",
    "\t\t\ttest_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "\t\t\tnum_workers=NUM_WORKERS, pin_memory=PIN_MEMORY\n",
    "\t\t)\n",
    "\n",
    "\t\t# === 5. Inicializar Modelo, P√©rdida y Optimizador ===\n",
    "\t\t\n",
    "\t\tmodel = ECG_1D_CNN(num_classes=NUM_CLASSES).to(DEVICE)\n",
    "\t\t\n",
    "\t\tloss_sup = FocalLoss(gamma=CURRENT_GAMMA, alpha=None, reduction='mean').to(DEVICE)\n",
    "\t\tloss_unsup = FocalLoss(gamma=CURRENT_GAMMA, alpha=None, reduction='none').to(DEVICE)\n",
    "\t\t\n",
    "\t\toptimizer = torch.optim.Adam(model.parameters(), lr=CURRENT_LR)\n",
    "\n",
    "\t\t# === 6. Bucle de Entrenamiento (Inner Loop) ===\n",
    "\t\t\n",
    "\t\tbest_f1_run = -1.0\n",
    "\t\t# Guardar el mejor modelo para esta \"run\" espec√≠fica\n",
    "\t\trun_model_path = f\"checkpoints/run_{run_id}_best_model.pth\" \n",
    "\t\tos.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "\t\tfor epoch in range(EPOCHS):\n",
    "\t\t\tavg_loss_s, avg_loss_u = train_fixmatch(\n",
    "\t\t\t\tmodel, labeled_loader, unlabeled_loader, optimizer, \n",
    "\t\t\t\tloss_sup, loss_unsup, DEVICE, CURRENT_THRESHOLD, CURRENT_LAMBDA_U\n",
    "\t\t\t)\n",
    "\t\t\t\n",
    "\t\t\tval_acc, val_f1, val_cm = evaluate(model, val_loader, DEVICE)\n",
    "\t\t\t\n",
    "\t\t\tif val_f1 > best_f1_run:\n",
    "\t\t\t\tbest_f1_run = val_f1\n",
    "\t\t\t\ttorch.save(model.state_dict(), run_model_path)\n",
    "\t\t\n",
    "\t\tprint(f\"Run {run_id+1} completada. Mejor F1-Macro de Validaci√≥n: {best_f1_run:.4f}\")\n",
    "\t\t\n",
    "\t\t# Guardar los resultados de esta \"run\"\n",
    "\t\tall_results.append({\n",
    "\t\t\t'run_id': run_id,\n",
    "\t\t\t'params': params,\n",
    "\t\t\t'best_val_f1': best_f1_run,\n",
    "\t\t\t'model_path': run_model_path\n",
    "\t\t})\n",
    "\n",
    "\t# === 7. Encontrar y Reportar los Mejores Resultados ===\n",
    "\n",
    "\tprint(\"\\n--- B√∫squeda de Hiperpar√°metros Finalizada ---\")\n",
    "\n",
    "\t# Convertir a DataFrame de Pandas para ver f√°cil\n",
    "\tresults_df = pd.DataFrame(all_results)\n",
    "\tresults_df = results_df.sort_values(by='best_val_f1', ascending=False)\n",
    "\n",
    "\tprint(\"Resultados de todas las 'runs':\")\n",
    "\tprint(results_df)\n",
    "\n",
    "\t# Obtener la mejor \"run\"\n",
    "\tbest_run = results_df.iloc[0]\n",
    "\n",
    "\tprint(\"\\n--- üèÜ MEJOR RUN üèÜ ---\")\n",
    "\tprint(f\"Mejor F1-Macro (Validaci√≥n): {best_run['best_val_f1']:.4f}\")\n",
    "\tprint(f\"Mejores Par√°metros: {best_run['params']}\")\n",
    "\tprint(f\"Mejor modelo guardado en: {best_run['model_path']}\")\n",
    "\n",
    "\t# === 8. Evaluaci√≥n Final y CSV con el MEJOR modelo ===\n",
    "\n",
    "\tprint(\"\\nCargando el MEJOR modelo para evaluaci√≥n final en Test...\")\n",
    "\n",
    "\t# Cargar el mejor modelo de la mejor \"run\"\n",
    "\tbest_model = ECG_1D_CNN(num_classes=NUM_CLASSES).to(DEVICE)\n",
    "\tbest_model.load_state_dict(torch.load(best_run['model_path']))\n",
    "\n",
    "\ttest_acc, test_f1, test_cm = evaluate(best_model, test_loader, DEVICE)\n",
    "\tprint(f\"\\n--- Resultados Finales (Test Set) con el Mejor Modelo ---\")\n",
    "\tprint(f\"Accuracy: {test_acc:.4f}\")\n",
    "\tprint(f\"F1-Macro: {test_f1:.4f}\")\n",
    "\tprint(\"Matriz de Confusi√≥n (Test):\")\n",
    "\tprint(test_cm)\n",
    "\n",
    "\t# --- 8.1 Generaci√≥n del Archivo de Submission (Test) ---\n",
    "\tprint(\"\\n--- Generando archivo CSV de submission ---\")\n",
    "\n",
    "\t# --- 8.2 Cargar los IDs del CSV de test ---\n",
    "\ttry:\n",
    "\t\ttest_df_original = pd.read_csv(\"ecg_signals/test_semi_supervised.csv\")\n",
    "\t\ttest_ids = test_df_original.iloc[:, 0].values\n",
    "\t\tprint(f\"IDs de test cargados: {len(test_ids)} encontrados.\")\n",
    "\texcept FileNotFoundError:\n",
    "\t\tprint(\"Error: No se pudo cargar 'test_semi_supervised.csv' para obtener los IDs.\")\n",
    "\t\texit()\n",
    "\t\t\n",
    "\t# --- 8.3 Obtener todas las predicciones del modelo ---\n",
    "\tbest_model.eval()\n",
    "\tall_predictions = []\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tfor x_batch, y_true_labels in test_loader:\n",
    "\t\t\tx_batch = x_batch.to(DEVICE)\n",
    "\t\t\t\n",
    "\t\t\tlogits = best_model(x_batch)\n",
    "\t\t\t\n",
    "\t\t\tpreds = torch.argmax(logits, dim=1)\n",
    "\n",
    "\t\t\tall_predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "\tprint(f\"Predicciones generadas: {len(all_predictions)} hechas.\")\n",
    "\n",
    "\t# --- 8.4 Verificar y Crear el DataFrame ---\n",
    "\tif len(test_ids) != len(all_predictions):\n",
    "\t\tprint(f\"¬°Error! La cantidad de IDs ({len(test_ids)}) no coincide \"\n",
    "\t\t\tf\"con la cantidad de predicciones ({len(all_predictions)}).\")\n",
    "\t\tprint(\"Verifica que el test_loader NO est√© barajando (shuffle=False).\")\n",
    "\telse:\n",
    "\t\tsubmission_df = pd.DataFrame({\n",
    "\t\t\t'ID': test_ids,\n",
    "\t\t\t'label': all_predictions\n",
    "\t\t})\n",
    "\n",
    "\t\toutput_filename = f\"ecg_submittions/submission_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.csv\"\n",
    "\t\t\n",
    "\t\tsubmission_df.to_csv(output_filename, index=False)\n",
    "\t\t\n",
    "\t\tprint(f\"\\n¬°√âxito! Archivo de submission guardado en: {output_filename}\")\n",
    "\t\tprint(\"Vista previa de los resultados:\")\n",
    "\t\tprint(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
